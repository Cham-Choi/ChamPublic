{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "322fcd75",
   "metadata": {},
   "source": [
    "## Digits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87facd7b",
   "metadata": {},
   "source": [
    "## 1. 모듈 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f193a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits   ## 데이터 지정\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b21300a",
   "metadata": {},
   "source": [
    "## 2. 데이터 준비, 3. 데이터 이해하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2715101e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< Feature names >\n",
      "['pixel_0_0', 'pixel_0_1', 'pixel_0_2', 'pixel_0_3', 'pixel_0_4', 'pixel_0_5', 'pixel_0_6', 'pixel_0_7', 'pixel_1_0', 'pixel_1_1', 'pixel_1_2', 'pixel_1_3', 'pixel_1_4', 'pixel_1_5', 'pixel_1_6', 'pixel_1_7', 'pixel_2_0', 'pixel_2_1', 'pixel_2_2', 'pixel_2_3', 'pixel_2_4', 'pixel_2_5', 'pixel_2_6', 'pixel_2_7', 'pixel_3_0', 'pixel_3_1', 'pixel_3_2', 'pixel_3_3', 'pixel_3_4', 'pixel_3_5', 'pixel_3_6', 'pixel_3_7', 'pixel_4_0', 'pixel_4_1', 'pixel_4_2', 'pixel_4_3', 'pixel_4_4', 'pixel_4_5', 'pixel_4_6', 'pixel_4_7', 'pixel_5_0', 'pixel_5_1', 'pixel_5_2', 'pixel_5_3', 'pixel_5_4', 'pixel_5_5', 'pixel_5_6', 'pixel_5_7', 'pixel_6_0', 'pixel_6_1', 'pixel_6_2', 'pixel_6_3', 'pixel_6_4', 'pixel_6_5', 'pixel_6_6', 'pixel_6_7', 'pixel_7_0', 'pixel_7_1', 'pixel_7_2', 'pixel_7_3', 'pixel_7_4', 'pixel_7_5', 'pixel_7_6', 'pixel_7_7']\n",
      "\n",
      "< Target names >\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "\n",
      "< Data shape >\n",
      "(1797, 64)\n",
      "\n",
      "< Label shape >\n",
      "(1797,)\n",
      ".. _digits_dataset:\n",
      "\n",
      "Optical recognition of handwritten digits dataset\n",
      "--------------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 1797\n",
      "    :Number of Attributes: 64\n",
      "    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\n",
      "    :Missing Attribute Values: None\n",
      "    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n",
      "    :Date: July; 1998\n",
      "\n",
      "This is a copy of the test set of the UCI ML hand-written digits datasets\n",
      "https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n",
      "\n",
      "The data set contains images of hand-written digits: 10 classes where\n",
      "each class refers to a digit.\n",
      "\n",
      "Preprocessing programs made available by NIST were used to extract\n",
      "normalized bitmaps of handwritten digits from a preprinted form. From a\n",
      "total of 43 people, 30 contributed to the training set and different 13\n",
      "to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n",
      "4x4 and the number of on pixels are counted in each block. This generates\n",
      "an input matrix of 8x8 where each element is an integer in the range\n",
      "0..16. This reduces dimensionality and gives invariance to small\n",
      "distortions.\n",
      "\n",
      "For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\n",
      "T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\n",
      "L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n",
      "1994.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n",
      "    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n",
      "    Graduate Studies in Science and Engineering, Bogazici University.\n",
      "  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n",
      "  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n",
      "    Linear dimensionalityreduction using relevance weighted LDA. School of\n",
      "    Electrical and Electronic Engineering Nanyang Technological University.\n",
      "    2005.\n",
      "  - Claudio Gentile. A New Approximate Maximal Margin Classification\n",
      "    Algorithm. NIPS. 2000.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "digits = load_digits()\n",
    "digits_data = digits.data\n",
    "digits_label = digits.target\n",
    "print(\"< Feature names >\")\n",
    "print(digits.feature_names)\n",
    "print(\"\\n< Target names >\")\n",
    "print(digits.target_names)\n",
    "print(\"\\n< Data shape >\")\n",
    "print(digits_data.shape)\n",
    "print(\"\\n< Label shape >\")\n",
    "print(digits_label.shape)\n",
    "\n",
    "print(digits.DESCR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3464a42",
   "metadata": {},
   "source": [
    "## 4. train, test 데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4207b65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(digits_data, digits_label, \n",
    "                                                    test_size = 0.5, random_state= 8)  ## test_size를 변경."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b10e21",
   "metadata": {},
   "source": [
    "## 5. 모델별 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "756613df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------Train result------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96        89\n",
      "           1       0.80      0.81      0.80        91\n",
      "           2       0.77      0.90      0.83        88\n",
      "           3       0.82      0.82      0.82        95\n",
      "           4       0.87      0.86      0.86        91\n",
      "           5       0.96      0.83      0.89        96\n",
      "           6       0.92      0.94      0.93        81\n",
      "           7       0.83      0.86      0.84        91\n",
      "           8       0.83      0.68      0.75        93\n",
      "           9       0.83      0.86      0.84        84\n",
      "\n",
      "    accuracy                           0.85       899\n",
      "   macro avg       0.86      0.86      0.85       899\n",
      "weighted avg       0.86      0.85      0.85       899\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier   ## 모델 불러오기\n",
    "\n",
    "decision_tree = DecisionTreeClassifier(random_state = 32)  ## 모델 지정\n",
    "decision_tree.fit(X_train, y_train)\n",
    "y_pred = decision_tree.predict(X_test)\n",
    "\n",
    "print(\"------------------------------Train result------------------------------\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2495040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------Train result------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99        89\n",
      "           1       0.95      0.97      0.96        91\n",
      "           2       0.96      0.99      0.97        88\n",
      "           3       1.00      0.96      0.98        95\n",
      "           4       1.00      0.97      0.98        91\n",
      "           5       0.96      0.94      0.95        96\n",
      "           6       1.00      0.99      0.99        81\n",
      "           7       0.99      1.00      0.99        91\n",
      "           8       0.96      0.92      0.94        93\n",
      "           9       0.91      0.96      0.94        84\n",
      "\n",
      "    accuracy                           0.97       899\n",
      "   macro avg       0.97      0.97      0.97       899\n",
      "weighted avg       0.97      0.97      0.97       899\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier   ## 모델 불러오기\n",
    "\n",
    "decision_tree = RandomForestClassifier(random_state = 32)   ## 모델 지정\n",
    "decision_tree.fit(X_train, y_train)\n",
    "y_pred = decision_tree.predict(X_test)\n",
    "\n",
    "print(\"------------------------------Train result------------------------------\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a82e32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------Train result------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        89\n",
      "           1       0.95      1.00      0.97        91\n",
      "           2       1.00      1.00      1.00        88\n",
      "           3       0.99      0.99      0.99        95\n",
      "           4       1.00      0.98      0.99        91\n",
      "           5       0.98      0.99      0.98        96\n",
      "           6       1.00      1.00      1.00        81\n",
      "           7       1.00      1.00      1.00        91\n",
      "           8       0.97      0.94      0.95        93\n",
      "           9       0.99      0.98      0.98        84\n",
      "\n",
      "    accuracy                           0.99       899\n",
      "   macro avg       0.99      0.99      0.99       899\n",
      "weighted avg       0.99      0.99      0.99       899\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Support Vector Machine\n",
    "from sklearn import svm   ## 모델 불러오기\n",
    "\n",
    "decision_tree = svm.SVC(random_state = 32)   ## 모델 지정\n",
    "decision_tree.fit(X_train, y_train)\n",
    "y_pred = decision_tree.predict(X_test)\n",
    "\n",
    "print(\"------------------------------Train result------------------------------\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "752a8038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------Train result------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        89\n",
      "           1       0.95      0.86      0.90        91\n",
      "           2       0.96      0.98      0.97        88\n",
      "           3       0.93      0.99      0.96        95\n",
      "           4       1.00      0.97      0.98        91\n",
      "           5       0.96      0.95      0.95        96\n",
      "           6       0.99      0.99      0.99        81\n",
      "           7       0.99      0.99      0.99        91\n",
      "           8       0.82      0.91      0.86        93\n",
      "           9       0.96      0.89      0.93        84\n",
      "\n",
      "    accuracy                           0.95       899\n",
      "   macro avg       0.96      0.95      0.95       899\n",
      "weighted avg       0.95      0.95      0.95       899\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Stochastic Gradient Descent\n",
    "from sklearn.linear_model import SGDClassifier   ## 모델 불러오기\n",
    "\n",
    "decision_tree = SGDClassifier(random_state = 32)   ## 모델 지정\n",
    "decision_tree.fit(X_train, y_train)\n",
    "y_pred = decision_tree.predict(X_test)\n",
    "\n",
    "print(\"------------------------------Train result------------------------------\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c27d266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------Train result------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        89\n",
      "           1       0.94      0.97      0.95        91\n",
      "           2       0.99      0.98      0.98        88\n",
      "           3       0.99      0.98      0.98        95\n",
      "           4       0.99      0.98      0.98        91\n",
      "           5       0.95      0.92      0.93        96\n",
      "           6       0.99      1.00      0.99        81\n",
      "           7       0.96      1.00      0.98        91\n",
      "           8       0.97      0.89      0.93        93\n",
      "           9       0.91      0.96      0.94        84\n",
      "\n",
      "    accuracy                           0.97       899\n",
      "   macro avg       0.97      0.97      0.97       899\n",
      "weighted avg       0.97      0.97      0.97       899\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression   ## 모델 불러오기\n",
    "\n",
    "decision_tree = LogisticRegression(random_state = 32, max_iter = 5000)   ## 모델 지정\n",
    "decision_tree.fit(X_train, y_train)\n",
    "y_pred = decision_tree.predict(X_test)\n",
    "\n",
    "print(\"------------------------------Train result------------------------------\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705a51e1",
   "metadata": {},
   "source": [
    "## 6. 평가 및 회고\n",
    "\n",
    "#### DT: Decision Tree, RF: Random Forest, SVM: Support Vector Machine, SGD: Stochastic Gradient Descent, LR: Logistic Regression\n",
    "### 모델별 예측정확도 결과는 아래와 같다.\n",
    "\n",
    "#### Test size: 0.1일때\n",
    "#### DT: 82%, RF: 98%, SVM: 98%, SGD: 98%, LR: 97%\n",
    "\n",
    "#### Test size: 0.2일때\n",
    "#### DT: 86%, RF: 97%, SVM: 98%, SGD: 95%, LR: 96%\n",
    "\n",
    "#### Test size: 0.3일때\n",
    "#### DT: 84%, RF: 97%, SVM: 99%, SGD: 95%, LR: 97%\n",
    "\n",
    "#### Test size: 0.4일때\n",
    "#### DT: 85%, RF: 97%, SVM: 99%, SGD: 95%, LR: 97%\n",
    "\n",
    "#### Test size: 0.5일때\n",
    "#### DT: 85%, RF: 97%, SVM: 99%, SGD: 95%, LR: 97%\n",
    "\n",
    "### SVM이 Test size에 관계없이 가장 높았다. 베스트 모델. 하지만 DT를 제외한 나머지도 95% 이상으로 나쁘지 않았다.\n",
    "### 이 데이터셋은 Test size에 영향을 받는 폭이 크지 않다. 모두 정확도가 높은 편. 2~3%차이는 알고리즘의 차이로 구분하기 쉽지 않다.\n",
    "### DT의 경우 들쭉날쭉했는데, 경로의 무작위성 영향을 많이 받는 것처럼 보였다. \n",
    "### RF는 DT에 비해 안정적이고 효과적으로 보인다. 아무래도 DT를 무작위적으로 여러 번 수행하는 특성 때문에 그런 것으로 보인다.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
