{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d13d6dc",
   "metadata": {},
   "source": [
    "## Wine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1309884",
   "metadata": {},
   "source": [
    "## 1. 모듈 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d923555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine   ## 데이터 지정\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e213d7d2",
   "metadata": {},
   "source": [
    "## 2. 데이터 준비, 3. 데이터 이해하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "956bc5f5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< Feature names >\n",
      "['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n",
      "\n",
      "< Target names >\n",
      "['class_0' 'class_1' 'class_2']\n",
      "\n",
      "< Data shape >\n",
      "(178, 13)\n",
      "\n",
      "< Label shape >\n",
      "(178,)\n",
      ".. _wine_dataset:\n",
      "\n",
      "Wine recognition dataset\n",
      "------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 178 (50 in each of three classes)\n",
      "    :Number of Attributes: 13 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      " \t\t- Alcohol\n",
      " \t\t- Malic acid\n",
      " \t\t- Ash\n",
      "\t\t- Alcalinity of ash  \n",
      " \t\t- Magnesium\n",
      "\t\t- Total phenols\n",
      " \t\t- Flavanoids\n",
      " \t\t- Nonflavanoid phenols\n",
      " \t\t- Proanthocyanins\n",
      "\t\t- Color intensity\n",
      " \t\t- Hue\n",
      " \t\t- OD280/OD315 of diluted wines\n",
      " \t\t- Proline\n",
      "\n",
      "    - class:\n",
      "            - class_0\n",
      "            - class_1\n",
      "            - class_2\n",
      "\t\t\n",
      "    :Summary Statistics:\n",
      "    \n",
      "    ============================= ==== ===== ======= =====\n",
      "                                   Min   Max   Mean     SD\n",
      "    ============================= ==== ===== ======= =====\n",
      "    Alcohol:                      11.0  14.8    13.0   0.8\n",
      "    Malic Acid:                   0.74  5.80    2.34  1.12\n",
      "    Ash:                          1.36  3.23    2.36  0.27\n",
      "    Alcalinity of Ash:            10.6  30.0    19.5   3.3\n",
      "    Magnesium:                    70.0 162.0    99.7  14.3\n",
      "    Total Phenols:                0.98  3.88    2.29  0.63\n",
      "    Flavanoids:                   0.34  5.08    2.03  1.00\n",
      "    Nonflavanoid Phenols:         0.13  0.66    0.36  0.12\n",
      "    Proanthocyanins:              0.41  3.58    1.59  0.57\n",
      "    Colour Intensity:              1.3  13.0     5.1   2.3\n",
      "    Hue:                          0.48  1.71    0.96  0.23\n",
      "    OD280/OD315 of diluted wines: 1.27  4.00    2.61  0.71\n",
      "    Proline:                       278  1680     746   315\n",
      "    ============================= ==== ===== ======= =====\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: class_0 (59), class_1 (71), class_2 (48)\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "This is a copy of UCI ML Wine recognition datasets.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
      "\n",
      "The data is the results of a chemical analysis of wines grown in the same\n",
      "region in Italy by three different cultivators. There are thirteen different\n",
      "measurements taken for different constituents found in the three types of\n",
      "wine.\n",
      "\n",
      "Original Owners: \n",
      "\n",
      "Forina, M. et al, PARVUS - \n",
      "An Extendible Package for Data Exploration, Classification and Correlation. \n",
      "Institute of Pharmaceutical and Food Analysis and Technologies,\n",
      "Via Brigata Salerno, 16147 Genoa, Italy.\n",
      "\n",
      "Citation:\n",
      "\n",
      "Lichman, M. (2013). UCI Machine Learning Repository\n",
      "[https://archive.ics.uci.edu/ml]. Irvine, CA: University of California,\n",
      "School of Information and Computer Science. \n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "  (1) S. Aeberhard, D. Coomans and O. de Vel, \n",
      "  Comparison of Classifiers in High Dimensional Settings, \n",
      "  Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of  \n",
      "  Mathematics and Statistics, James Cook University of North Queensland. \n",
      "  (Also submitted to Technometrics). \n",
      "\n",
      "  The data was used with many others for comparing various \n",
      "  classifiers. The classes are separable, though only RDA \n",
      "  has achieved 100% correct classification. \n",
      "  (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) \n",
      "  (All results using the leave-one-out technique) \n",
      "\n",
      "  (2) S. Aeberhard, D. Coomans and O. de Vel, \n",
      "  \"THE CLASSIFICATION PERFORMANCE OF RDA\" \n",
      "  Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of \n",
      "  Mathematics and Statistics, James Cook University of North Queensland. \n",
      "  (Also submitted to Journal of Chemometrics).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wine = load_wine()\n",
    "wine_data = wine.data\n",
    "wine_label = wine.target\n",
    "wine.target_names\n",
    "print(\"< Feature names >\")\n",
    "print(wine.feature_names)\n",
    "print(\"\\n< Target names >\")\n",
    "print(wine.target_names)\n",
    "print(\"\\n< Data shape >\")\n",
    "print(wine_data.shape)\n",
    "print(\"\\n< Label shape >\")\n",
    "print(wine_label.shape)\n",
    "print(wine.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47388dd",
   "metadata": {},
   "source": [
    "## 4. train, test 데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a54d05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(wine_data, wine_label, \n",
    "                                                    test_size = 0.5, random_state= 8)   ## test_size를 변경해가며 시험."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1be336a",
   "metadata": {},
   "source": [
    "## 5. 모델별 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a1f6ca6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------Train result------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.87      0.93        30\n",
      "           1       0.85      0.97      0.91        36\n",
      "           2       0.95      0.91      0.93        23\n",
      "\n",
      "    accuracy                           0.92        89\n",
      "   macro avg       0.94      0.92      0.92        89\n",
      "weighted avg       0.93      0.92      0.92        89\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier   ## 모델 불러오기\n",
    "\n",
    "decision_tree = DecisionTreeClassifier(random_state = 32)   ## 모델 지정\n",
    "decision_tree.fit(X_train, y_train)\n",
    "y_pred = decision_tree.predict(X_test)\n",
    "\n",
    "print(\"------------------------------Train result------------------------------\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "274897e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------Train result------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        30\n",
      "           1       1.00      1.00      1.00        36\n",
      "           2       1.00      1.00      1.00        23\n",
      "\n",
      "    accuracy                           1.00        89\n",
      "   macro avg       1.00      1.00      1.00        89\n",
      "weighted avg       1.00      1.00      1.00        89\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier   ## 모델 불러오기\n",
    "\n",
    "decision_tree = RandomForestClassifier(random_state = 32)   ## 모델 지정\n",
    "decision_tree.fit(X_train, y_train)\n",
    "y_pred = decision_tree.predict(X_test)\n",
    "\n",
    "print(\"------------------------------Train result------------------------------\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c609c4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------Train result------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.90      0.89        30\n",
      "           1       0.59      0.94      0.72        36\n",
      "           2       1.00      0.00      0.00        23\n",
      "\n",
      "    accuracy                           0.69        89\n",
      "   macro avg       0.82      0.61      0.54        89\n",
      "weighted avg       0.79      0.69      0.59        89\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Support Vector Machine\n",
    "from sklearn import svm   ## 모델 불러오기\n",
    "\n",
    "decision_tree = svm.SVC(random_state = 32)   ## 모델 지정\n",
    "decision_tree.fit(X_train, y_train)\n",
    "y_pred = decision_tree.predict(X_test)\n",
    "\n",
    "print(\"------------------------------Train result------------------------------\")\n",
    "print(classification_report(y_test, y_pred, zero_division=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99109e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------Train result------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.73      0.85        30\n",
      "           1       0.83      0.28      0.42        36\n",
      "           2       0.38      0.91      0.54        23\n",
      "\n",
      "    accuracy                           0.60        89\n",
      "   macro avg       0.74      0.64      0.60        89\n",
      "weighted avg       0.77      0.60      0.59        89\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Stochastic Gradient Descent\n",
    "from sklearn.linear_model import SGDClassifier   ## 모델 불러오기\n",
    "\n",
    "decision_tree = SGDClassifier(random_state = 32)   ## 모델 지정\n",
    "decision_tree.fit(X_train, y_train)\n",
    "y_pred = decision_tree.predict(X_test)\n",
    "\n",
    "print(\"------------------------------Train result------------------------------\")\n",
    "print(classification_report(y_test, y_pred, zero_division=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ba1fac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------Train result------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.90      0.93        30\n",
      "           1       0.89      0.94      0.92        36\n",
      "           2       0.96      0.96      0.96        23\n",
      "\n",
      "    accuracy                           0.93        89\n",
      "   macro avg       0.94      0.93      0.94        89\n",
      "weighted avg       0.93      0.93      0.93        89\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression   ## 모델 불러오기\n",
    "\n",
    "decision_tree = LogisticRegression(random_state = 32, max_iter=4000)   ## 모델 지정\n",
    "decision_tree.fit(X_train, y_train)\n",
    "y_pred = decision_tree.predict(X_test)\n",
    "\n",
    "print(\"------------------------------Train result------------------------------\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5745f8",
   "metadata": {},
   "source": [
    "## 6. 평가 및 회고\n",
    "\n",
    "#### DT: Decision Tree, RF: Random Forest, SVM: Support Vector Machine, SGD: Stochastic Gradient Descent, LR: Logistic Regression\n",
    "### 모델별 예측정확도 결과는 아래와 같다.\n",
    "\n",
    "#### Test size: 0.1일때\n",
    "#### DT: 83%, RF: 94%, SVM: 56%, SGD: 72%, LR: 83%\n",
    "\n",
    "#### Test size: 0.2일때\n",
    "#### DT: 94%, RF: 100%, SVM: 64%, SGD: 58%, LR: 92%\n",
    "\n",
    "#### Test size: 0.3일때\n",
    "#### DT: 87%, RF: 100%, SVM: 65%, SGD: 56%, LR: 93%\n",
    "\n",
    "#### Test size: 0.4일때\n",
    "#### DT: 97%, RF: 100%, SVM: 69%, SGD: 61%, LR: 93%\n",
    "\n",
    "#### Test size: 0.5일때\n",
    "#### DT: 92%, RF: 100%, SVM: 69%, SGD: 95%, LR: 97%\n",
    "\n",
    "### RF가 Test size에 관계없이 가장 높았고 0.2~0.5에서는 100%였다. 베스트 모델.\n",
    "### 이 데이터셋은 Test size에 영향을 받는 폭이 Digits에 비해 컸다. Feature number에 영향을 받는건가 싶은데.. 이유에 대한 근거가 부족하다. 추후 추가 스터디가 필요하다.\n",
    "### DT의 경우 들쭉날쭉했는데, 경로의 무작위성 영향을 많이 받는 것처럼 보였다. 이는 Digits에서의 추이와도 비슷하다.\n",
    "### RF는 DT에 비해 안정적으로 보인다. Test size 0.1 ->0.2로 갈 때 100%로 6% 상승한 후 계속 100%가 나왔다. 앞서 SVM이 베스트였던 것과는 달리 RF가 베스트인 것이 주목할만한 부분. SVM은 비선형 모델에서 정확도가 높다고 하는데, 이 데이터셋은 데이터가 선형성이 더 두드러져서 그런 것이 아닌가 싶다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
